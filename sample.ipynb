{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from datasets.datasetgetter import get_dataset, Compose\n",
    "from datasets.custom_dataset import DatasetDumped, ImageFolderRemap\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import to_pil_image, to_tensor\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "obj_path = '../font-dumped/characters.obj'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "mean = [0.5]\n",
    "std = [0.5]\n",
    "normalize = transforms.Normalize(mean=mean, std=std)\n",
    "img_size = 80\n",
    "transform = Compose([transforms.Resize((img_size, img_size)),\n",
    "                         transforms.ToTensor(),\n",
    "                         normalize])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 100000 examples\n",
      "processed 200000 examples\n",
      "processed 300000 examples\n",
      "unpickled total 312780 examples\n"
     ]
    }
   ],
   "source": [
    "dataset = DatasetDumped(obj_path, transform, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "        drop_last=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.24093008041382\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for (img, target_id) in iter(data_loader):\n",
    "    pass\n",
    "end = time.time()\n",
    "print(end - start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "img_dir = '../font-images'\n",
    "remap_table = {i: i for i in range(200)}\n",
    "img_dataset = ImageFolderRemap(img_dir, transform=transform, remap_table=remap_table, input_ch=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "img_data_loader = torch.utils.data.DataLoader(\n",
    "    img_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    drop_last=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.31427192687988\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "count = 0\n",
    "for (img, target_id) in iter(img_data_loader):\n",
    "    count += 1\n",
    "    pass\n",
    "end = time.time()\n",
    "print(end - start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9775\n"
     ]
    }
   ],
   "source": [
    "print(count)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "img = Image.open('../font-images/id_0/0000.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "rgb_img = img.convert('RGB')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "<PIL.Image.Image image mode=L size=80x80>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFAAAABQCAAAAACreq1xAAAEe0lEQVR4nO2YbUyVZRjHf+d4IHk7igps0EAHZIDgFJGaSrIiUqBcL7PpXMmYUG290KLV/JA5adUHwU3SpZKN2spmy5o1060WZtkL8mJm2ZBRc0BCvMab5+rDc16f5znnPjD6UOP6cs51Pf/791zPdd/X/dznWISZNesM82aBs8D/CPB6y8wCO9KWV0ypPUVh+4BXVCIvUwKbQyByMHig8pGzXoKhC8E/sXpS1gAjMwkcAxwzCewAYl3OxLSAE+viX/SslF+BFO3ryeXhzyuJJhN1Hmhwe5sgVkREviwAuDaNZdMMZI27vAy4Q2TgSC4ASWPTADrSgI2/OZ25UFy3KUwr0JZOBU8sZm11Ln8MyFq/OGFi8HKNJ16yJ1NZQvNOOT7HKAwva1Nl5++RReSrRF9aRMnbQbafv14erS9c4MaVn1FNhcdMa+i0rt5+W/j+Ohi9SV06l9kCXIuLAwTmTYGnbr1eiPb2D25uDqgPlCEAfRDl5Z6u4GJbIL0yw36I9HJ/gIt/BtIrMxzQgIM/tnRcvT48eAUoXrtiZZrfAaplkAjFZ55dpn+SJVU/TW0dui3OXyaWB1umAzzuSc1y84q75wPLFjvrZKu+MVVg051OWHp5Q9OISIsVEkZl9PwurYYbRqcE7CnT0kt6td0ZKQAOa18/ywF4wJBjAOCxGGd6Fa7Ih8Bqh9O58fIcYHfQwKHtANxi8QCHk8D6vUdy1AJh7bpx/oA/LwWI3jse6gFWAY95i14AHg0O+Kkd4P5rImFQrsUu2CCm11s1Eg/zdDubOfDdECDyHRGRKCgVEZHJVUC9r24PcCoIYIMVyLgsIiKLYJt78F064SWgWg38xAbk/6U5ibBZRKQ1FCLa9dIoeFgJvGQHNrhKcysUi8jfmcA+gzYdClXAsUwgb8Tl5kK+iDwBrDd22krIUwF3Akk9bvceyBF5D4g2ecmnwH0KYMdcsH7j8bdCqrRFAB8YeWMh8LgCuAOo9PIrYUH3Etyr0ccagUOBgb1hEN3vFXgdrLcBucaNReRJ4JfAwKPALu/AIW2DiP3dhNcXCRm6mH5rPw2W7d4B7ZUXeizBZNPePwSP6IO6G2RDtk/gBIDtfZP85Eo42Ht1QX2GnZDkE/gO4MhDJvlJ6QhURhvCvrYQdnj7dRZgqVl+8hyQPKyP6oEJPs05WgFAjBmvFrB+YQjrgXmQ7Ha+db3O2428vQC1xrge+AzgvG13mRUILQY+1w+bfAp8G8Af8GsgvU9EBnbbAeLPXQ0B/Smhcx3AVocYzdB6a4Hk6tpSOwBFXSIfFR3wlTjenA9QZcYzAlvDPCsg+oDZkLO3A9gOml0z275OuYih5d1muI0ApDaa80x37CKA1J0m3Tv81moArE+PGC/6BYr0NDZ2GaPjH2+J0HJfc9YfLojjnGaO1poSu7MU2ScDKYMA9r+xbZX7VBxy74nAajVwPMU965acGrN58jHlGZvJP7TP+ILCgkVKtfqMLVK/0JJQ/FqTWigigX+aTcv+D//OzQJngf8+8B/EE70tuQYwQAAAAABJRU5ErkJggg==\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(rgb_img)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "PIL.Image.Image"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rgb_img)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "args = type(\"Piyo\", (object,), {\"img_size\": 80, \"att_to_use\": [0,1,2,3], \"data_dir\": \"../font-images\", \"val_num\": 10, \"input_ch\": 1})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE CLASSES [0, 1, 2, 3]\n",
      "LABEL MAP: {0: 0, 1: 1, 2: 2, 3: 3}\n",
      "MINIMUM TRAIN DATA FONT : 2214\n",
      "MAXIMUM TAIN DATA FONT : 2218\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset = get_dataset(args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "seek of closed file",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [4], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m val_loader \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mDataLoader(val_dataset)\n\u001B[1;32m      3\u001B[0m it \u001B[38;5;241m=\u001B[39m \u001B[38;5;28miter\u001B[39m(train_loader)\n\u001B[0;32m----> 4\u001B[0m imgs, y_org \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mit\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/learn-computer-vision/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    678\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    679\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    680\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 681\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    682\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    683\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    684\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    685\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/learn-computer-vision/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:721\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    719\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    720\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 721\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    722\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    723\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/learn-computer-vision/venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfetch\u001B[39m(\u001B[38;5;28mself\u001B[39m, possibly_batched_index):\n\u001B[1;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[0;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/learn-computer-vision/venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfetch\u001B[39m(\u001B[38;5;28mself\u001B[39m, possibly_batched_index):\n\u001B[1;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[0;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/learn-computer-vision/venv/lib/python3.9/site-packages/torch/utils/data/dataset.py:290\u001B[0m, in \u001B[0;36mSubset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m    288\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(idx, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m    289\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m idx]]\n\u001B[0;32m--> 290\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\n",
      "File \u001B[0;32m~/learn-computer-vision/DG-Font/datasets/custom_dataset.py:312\u001B[0m, in \u001B[0;36mImageFolderRemap.__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m    310\u001B[0m sample \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloader(path, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_ch)\n\u001B[1;32m    311\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 312\u001B[0m     sample \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43msample\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    313\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    314\u001B[0m     target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform(target)\n",
      "File \u001B[0;32m~/learn-computer-vision/DG-Font/datasets/datasetgetter.py:14\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtf:\n\u001B[0;32m---> 14\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[0;32m~/learn-computer-vision/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/learn-computer-vision/venv/lib/python3.9/site-packages/torchvision/transforms/transforms.py:349\u001B[0m, in \u001B[0;36mResize.forward\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m    341\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[1;32m    342\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    343\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m    344\u001B[0m \u001B[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    347\u001B[0m \u001B[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001B[39;00m\n\u001B[1;32m    348\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 349\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minterpolation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mantialias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/learn-computer-vision/venv/lib/python3.9/site-packages/torchvision/transforms/functional.py:430\u001B[0m, in \u001B[0;36mresize\u001B[0;34m(img, size, interpolation, max_size, antialias)\u001B[0m\n\u001B[1;32m    428\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    429\u001B[0m     pil_interpolation \u001B[38;5;241m=\u001B[39m pil_modes_mapping[interpolation]\n\u001B[0;32m--> 430\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF_pil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minterpolation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpil_interpolation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    432\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m F_t\u001B[38;5;241m.\u001B[39mresize(img, size\u001B[38;5;241m=\u001B[39msize, interpolation\u001B[38;5;241m=\u001B[39minterpolation\u001B[38;5;241m.\u001B[39mvalue, max_size\u001B[38;5;241m=\u001B[39mmax_size, antialias\u001B[38;5;241m=\u001B[39mantialias)\n",
      "File \u001B[0;32m~/learn-computer-vision/venv/lib/python3.9/site-packages/torchvision/transforms/functional_pil.py:282\u001B[0m, in \u001B[0;36mresize\u001B[0;34m(img, size, interpolation, max_size)\u001B[0m\n\u001B[1;32m    277\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m max_size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    278\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    279\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_size should only be passed if size specifies the length of the smaller edge, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    280\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mi.e. size should be an int or a sequence of length 1 in torchscript mode.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    281\u001B[0m     )\n\u001B[0;32m--> 282\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\u001B[43msize\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minterpolation\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/learn-computer-vision/venv/lib/python3.9/site-packages/PIL/Image.py:2046\u001B[0m, in \u001B[0;36mImage.resize\u001B[0;34m(self, size, resample, box, reducing_gap)\u001B[0m\n\u001B[1;32m   2042\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreducing_gap must be 1.0 or greater\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2044\u001B[0m size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(size)\n\u001B[0;32m-> 2046\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2047\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m box \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2048\u001B[0m     box \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msize\n",
      "File \u001B[0;32m~/learn-computer-vision/venv/lib/python3.9/site-packages/PIL/ImageFile.py:226\u001B[0m, in \u001B[0;36mImageFile.load\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    219\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtile \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    220\u001B[0m     \u001B[38;5;28mlist\u001B[39m(tiles)[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m    221\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m _, tiles \u001B[38;5;129;01min\u001B[39;00m itertools\u001B[38;5;241m.\u001B[39mgroupby(\n\u001B[1;32m    222\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtile, \u001B[38;5;28;01mlambda\u001B[39;00m tile: (tile[\u001B[38;5;241m0\u001B[39m], tile[\u001B[38;5;241m1\u001B[39m], tile[\u001B[38;5;241m3\u001B[39m])\n\u001B[1;32m    223\u001B[0m     )\n\u001B[1;32m    224\u001B[0m ]\n\u001B[1;32m    225\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m decoder_name, extents, offset, args \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtile:\n\u001B[0;32m--> 226\u001B[0m     \u001B[43mseek\u001B[49m\u001B[43m(\u001B[49m\u001B[43moffset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    227\u001B[0m     decoder \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39m_getdecoder(\n\u001B[1;32m    228\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode, decoder_name, args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoderconfig\n\u001B[1;32m    229\u001B[0m     )\n\u001B[1;32m    230\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[0;31mValueError\u001B[0m: seek of closed file"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset['TRAIN'], batch_size=8, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset)\n",
    "it = iter(train_loader)\n",
    "imgs, y_org = next(it)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGB size=80x80>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFAAAABQCAIAAAABc2X6AAAC6klEQVR4nO2bP0gyYRzHn+d4PJIgUBcn/wxxUw4NDg7OzsVthVMgNAQ5uAQt4dpgLoKgIu7qgbo1SYhgOBmVEkaTSw6WdpwNQkgSpP5+va+Pz2fy7/f5fuCe5+4elI7HY7JOSP+6wF8jhHlHCPOOEOYdIcw7Qph3EIWvr6+9Xu/BwUG/38cbZW7GOBwdHdlsNkoppfTw8LDb7SINNC90jHO3tLu7e3t7+/V0OByaTCaMgeYF5ZCuVqvTtoSQq6srjIEWAEV4dtK+vr5iDLQAYpXmHbBF6/T0lBBSKBR0Xdd1/eXlZfpdWZbtdvu8mYyxQCAwGo0SiQRISUKATkvJZNJsNlMcTCZTPB4H6TmGOi09PT253e7lc36i0+k4nU6QKJg5/P7+DpLzB/kwwpVKRZZlkKhZJEkqFotQaWCLVigUIoRomqbrumEYvV5v+l3GmNVqnTdTkqTJopXNZkFKEoJzLV0qlb4tPOfn5xgDLcDanYeFMO8IYd4RwrwjhHkHRZgxtnzIzc3N4+NjOp12uVyANw8AzWbx+XxbW1vTO1upVGrekFwu9/DwMHlcLBZVVYUph3TJenx8DLsNkM/nQYphzeFCoQAbqGkaSA6WcLvdnuxyQTEajUBysIQZY3t7e2dnZ0j5iwMyMX7i4+NDUZRoNOp2u5ecw8FgEKQSyir9BWOs1WoRQiKRyPPzs2EYv/xio9HY399HqYQROoskSQ6H4/efv7u7w2qClPvfIoR5RwjzjhDmHSHMO0KYd4Qw7whh3lkZYUmCqYq4AZDJZFRVbTabC3z38vLy2yt+vx+iFNqeVjgc3tzchNiQppRSi8Xy9vYGUgzrkJZleTAYQKXFYrGNjQ2QKCzher0OFeVyuba3t6HSsA7pWq0GcjArinJycgJYDEv4/v7+4uJiGdWdnZ1yudxoNGCLYf3nYUK1Wl1slfZ4PD6fD7wPAfzp4aqwMhceUAhh3hHCvCOEeUcI844Q5p21E/4El4g5GpzU6bgAAAAASUVORK5CYII=\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGB size=80x80>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGB size=80x80>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFAAAABQCAIAAAABc2X6AAACiUlEQVR4nO3Zz4sxcRzA8cGjPaCVSNIuNzkqP3oOklwUF7WuK/kDNmftRdmTkwtqipODOI3jnrdty5SDixCX3TGDg3LaYZ6DVnqe9bT7PPOd3T4+r5M0fD/vkZKPSpIk6pSov3oApWEwdBgMHQZDh8HQYTB0GAwdBkOHwdCdXPAPZY4Zj8cPDw9PT0/Pz8/8m9lsxh8QRbFcLjebTafTSXAUSW6CIFSr1XA4zPN8qVRKJBJWq1X1Yefn581mM5PJrNdr2WeTJEn+4EKhQNO0RqP5eOS73G53NpuVfTz5g3cYhhkOh4FA4FjP2dlZLBajaXq5XHo8np9vKpXK/ppGoyH7YKS+w/F4nKKox8fHTqfDsuxgMOA4jud5s9ns9Xr9fn80GtXr9buLWZbdv3A6ne4eXF5evry8yD+Z7LfwfwiCEAqFJpNJOp1eLBYkjlBJ3+yP+OVyaTKZyL3/twsm7eR+eGAwdBgMHQZDh8HQYTB0GAwdBkOHwdBhMHQYDB3BYFEUV6sVuff/N2T/pr2+vjYajaVSidwRn0U2eLVaXV1dJZPJi4uLaDRK7qBPILHOOJRKpVQqlcFguL+/z+Vym82G9Il/p8Ru6e7ubr8QDAaDoigyDKPAue9SIrjdbns8nt/WpYlEguO429tbQovvYxTaHvZ6vcPP+ZDD4Wi1WpFIpN/vKzCJcuvS19dXnU5nt9uPrci1Wu3NzY3NZiM6htLbw8FgYLFYXC7XfD5/9wK1Wk1RlCiKpCYgejuPGY1GPp8vn89rtdo/P+pOp0Pu6C/eD7MsWywWa7UaTdPdbnf3ZL1e3263hE7EhTh0GAwdBkOHwdBhMHQYDB0GQ4fB0GEwdBgM3ckF/wIf+ZRNfZVjyAAAAABJRU5ErkJggg==\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGB size=80x80>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFAAAABQCAIAAAABc2X6AAAHR0lEQVR4nO2af0hTXRjH73a3ZFNaWy0amZaThFYOEXHGZlkuhTBZRsMVSIiRacVaY1xCIgeF4ILqjzSIEYZBhWSBk35AaVltLctKW9P+mHNYtpxFq1hu7x8HDsu36d12rq/bu89fz+7O85zne889P+45lxYIBLD/E/T/OoH5JiE43kkIjncSguOdhOB4h4EqkNfrvXjxIo/H27Fjx5IlS1CFRU8AESaTSSwW02i05OTkxsZGsVg8OjqKKjhCaAF0a2mXy5WRkcFgMLxeL4Zhy5cvt1qtbrc7OzsbVRUIQH4LFQoFLQihUFhbW4u8lohBLzgQCNTW1gZrzs3NpaKWyKBEsNfrDRbc0dFBRS2RQcm0lJubC+2dO3eKxWIqaokMSgSDQQvDsLS0tIMHD2ZkZFBRS2SgHKUhU1NTXC4XwzCr1ZqTk4M8fjRQ0sLXrl0DxvHjx6mIHw2UCDYajcA4cOBAxEFgv0AM8mHQbreDwXnp0qU+ny9cd7fbXVJSotFocBzXaDR1dXVo00MvmCAIILimpoa81/T0tF6vN5vNq1atov2Jw+FAmB56wTDj7u5uMuWNRqPH45HJZLQQ5OTkTE9Po0ovDMETExNkivX09IBEmUxmqDIPHz4sKyt7+fJlUVFRKJ00Gq2qqorJZPb395NPck7ICr58+TKO493d3WNjY7OXrKmpAekqFApwxWazGQwGmUz26dOn5uZmkUgUSqFMJrty5QqLxVKr1UNDQ1EpCwFZwW/fvmWxWDQaraSk5K+vfm63++7du01NTVwuF2RPEER1dfWaNWtCyWOxWEqlsquri8/nEwRht9tRq/sLYSw87ty5AwRjGEan0zdu3CiRSDwez/j4+MDAgMPhmN0duCiVyt27d1dWVqpUql27di1evDjaaSZcwr1DarVaLpfP0vEAHA7nxIkTDx48wHG8rKystbX148ePFDRY2EQySn///n3fvn1fvnzp6+sbHBz0+/1ZWVkVFRV6vf79+/dAcGNjI/JckYB4WoJD9NOnT9FGRgXipeXt27eBAbr6AgSx4K6uLmCUlpaijYwKlIIdDsfg4CCwt23bhjAyQlAKhs2LYVhxcTHCyAhBKbinpwcYa9euTUtLQxgZISgFP3r0CBgLtnkxtIIfP34MDLlcTtLF5XIdO3aMTqebTKbDhw9LJJLVq1ez2ewVK1ZIJBKVSnX69Gm32713794XL16gyRLhFNfe3g4mYY/HM3tJi8UiFAp1Ol1SUtKcizYAjuNnzpyprq6OYFMhGJSC6+rqQHIbNmwIVUav17e3t8/QKZfLzWazTCYjCKKzs3NiYmJsbOz+/fsGg0EgELS0tHA4HFhYoVCcPXs24iRRCs7LywM5/XWvQ6vVHjp0KFhneXl5enp6b2/vnJEtFovD4YDvYUlJST9//oysqZEJ9vl8sN1aW1uD/zp//nxDQ0Ow1JaWFp1OF+4+htFo3Lp1Kwxy8+bNX79+hZsnMsH9/f0wlSdPnoCLQ0NDZrMZx3H41549ewiCiLiWkZGRqqoqGC2C7S5kgi9dugTzAJtBMplMKBTCiytXrhSJRBG0yQx6e3vBQTRAo9GE5Y7sC4A3b94Ag8FgDA8PX716FU7LGIadPHly06ZNhYWF0VcklUqzs7MHBgbAT6fT+fv3bwaDtJAo7zcE7grgOB681cpkMnEcj3Iu+TfBJ7Jh7V0jW3i8e/cOGH6/3+l0Ajs1NbW+vj68FiDHhw8foF1eXh6GJ6pb7vf7aX8iEona2tpQxZ/Bq1ev4FiYl5dH3hGZYKfTGayWw+FcuHABVfC/olQqYXUjIyMkvZA90suWLQv+OTo6Gs1JGhkKCgqgHTxAzg4ywc+ePUtNTQU2g8GwWCyoIodi3bp10B4fHyfphUxwYWFhfX39+vXrMQyzWq1btmxBFTkUwc8Um80m64a2X9lsNq1WG/3qggwdHR2wD3d2dpL0ouQrnvlBp9PBmX/OEy9IrAr2+XwCgQAIlsvl5B1j9Wva5uZmOFBpNBryjrEqWKFQQLutrS0MT+qeOup4/fo1n8+H6zm3203eNyZbOBAIfP78GdhOp5PH45H3jT3BR48ehd8ylpaWDg8Ph+UeY4Jv3Lhx69YtYC9atGjz5s0zlrRzQ1lHQ8/169fz8/PhYiM/Pz+CIDEjWKvVws9FwB5DZHFiQPDk5KTL5YJvv3w+X6lURhxtoQuurKwM3polCMJkMkUTkJLPh6PH7/fv378/JSXl3Llz4IpUKqXT6ffu3Yt2twhRSyDDYrFwudzt27fDVq2oqBCLxZOTk0jiLwjB3759MxgMycnJR44cgX01Kyvr+fPnDQ0NaOuaV8FTU1M2mw2ckimVSoFA0NfXp1ar4aFRenr6qVOnmExmU1PTjx8/qMhhnvqw3W4vKCj4+vVrSkoKm83m8XiZmZmZmZkikUgqlRYVFRUXF6tUqnk4SV+ggxZ1xNjSMnoSguOdhOB4JyE43kkIjnf+Af4EEHJInsLKAAAAAElFTkSuQmCC\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGB size=80x80>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFAAAABQCAIAAAABc2X6AAAHs0lEQVR4nO2ba0iTXxzHe3bPKXPJtlrYUjPogjVEpXQsGyVmEGK9yLGgGyJF5YtIqF5FCWKg5QulkpgVVHZ5MXBILCO1tBIxbSzXkuYoa9laaze3Pf8X/htr52x7bpv+/e/zSn6e3+X7O+d59uw5ZwiKokv+T9Dmu4BEkxS82EkKXuwkBS92koIXO0nBC4kjR44YDIYPHz40NzdTFhRdkPz8+bO5uRn5A5PJlEqllEReoILXrVuH/A2Xy+3s7CQfmciSttvt3759KywsFIvFDQ0NarWasvX2h6tXr4ZZnE4nnU6nIDSBJul0utDe5+TkvHnzhnzvQxkZGUEAuFyu2WwmGRn3DI+NjZWVlYVaTCbT0qVLKeh9CO3t7aDR6XS2traSDY23Q52dnWDvqbqjBPF4PJs2bQIT5efnk4yMW7DFYqHT6WAp4+PjJEsJo6OjA8wiEAhIhsW9pMVi8a5du0B7b28v2cX2N16vFzTSaGQfHIj4r1+/HjR+/fqVZClhWCwW0JidnU0yLBHB0N47nU7o4Orq6r179wqFwuPHjzc2NmLP8uXLF9C4devW4N9ardblcm3fvp3FYh06dOjEiROY4hK4DMrLy8Gr6+LFi+DIlpYWNpsdHDN3M7dYLFiylJWVgVl8Pp/Var1582ZBQUHYv9hsdl1dXcywDOwtDzIyMgIa8/LyQGNmZmbocnC73QiCrF27trCwcM2aNSkpKRwOh8FgMBj/luH1em02W2pqqtVq7enpAQMymcxIVXm93kir7C/wTu/4+DjYeARBpqenwcFVVVXQwXFi5cqVMevHfQ0/efIENGZnZwuFQtAul8vxxidDdXV1zDG4BT9+/Bg0btu2DTrYbDbjjU+Y1NTU5cuXxx6Hd0k7HA5wLd27dy/SeJFIlIDFXFpaCr1rguAWrFarwXw2my2KS0VFRXFxMeUi09LSamtrTSZTTU3Nr1+/MNaPW/CePXvA3MXFxdG9fvz4UVlZ2dvbu2PHDi6XS1ikTqdjMpmVlZW3b9/+/fs33uJRFEVQPLuHTqdTKBSCd/9Lly7V19djDBIIBEZHR9+/f2+3291ut9frDQQCNBqNRqMxGAwWi8XhcDgcTn19/eTkZJjv5OTkqlWrsBcMAVd7Hjx4AG3827dvCTQ7Ohs2bAATGQwGkmHx3aU1Gg1oXL169caNG0l1HQaLxQKNmB4tooJPcF9fH2jcvXs3ySKgBB+/QnG73STD4hPc398PGsvLy0kWASUlJQU0JlrwwMAAaNy5cyfJIqAsiCUNnWGFQkGyCCgcDgc0JnqGh4eHQWNJSQnJIqAsiCVtNBpBY+iXcgqBznBCl7TX652amgLtmzdvJlkElPmfYaPRGAgEwowcDkcsFpMsAsr8X8Mmkwk0xkntkoWwpK1WK2iMn2AulwsaEzrDNpsNNMZP8Pxfw1DB0Dc7lAAVnNAlbbfbQSP0eYgS5n+God2FPuJTwvzPMPiZtCThghM6w9CNrMUsOE5fXyIRpyWNY36gFZDZNHz9+vW5c+fa29v7+voMBoPP5+PxeHl5eSUlJfv27VMqlaAL+XskDsHp6emgEfo0EpNTp05lZGRcuHDB5/NlZWVBx0D3lpYtW0YgXSg4BEM/cvEKHhoaGhwcvHLlCi6vINDvp/jA/r6vu7sbfI24YsWKuf/Ozs7u37+fyWTW1NT4/X5ohEePHoHbnLjIyclRKpWR4mMBh+CJiQloEdPT02az+fz580HL6dOnwRe3Hz9+LCoqIqM2iEqlOnv2bNwF+/3+0N3tIN3d3Y2NjWFGtVod5g49ekUGrVY7MTERR8Eoiubn54OJP336BBr5fH6oo8ViEQgE1ApGEKSoqAjvVOMTXFtbC2aFTvuWLVtCHQ8ePAitWCAQPHz4UCKRVFVVqVQqhULB5XIHBgYijYdmD2sulYK7urow1vHu3btQx4yMDHCMTCZ79uxZpFyBQABjLgRBdDqdy+WiXrDT6cSSXiAQDA4OhjrOne4Iw+FwRMkFdYlCW1sblk1TfIKh53dAZmZmwhyhW6RdXV2REnk8HpVKhUswgiAdHR0ej4dKwXfv3sWS+MaNG2GOkaqfmpr6/Plz2OAXL17cuXMHOl4ulx8+fDhK6rS0NCoFz87OZmZmRlebm5sLPhjo9fpI++Aikaipqenly5ejo6Majebo0aPQuyCCIAUFBSiKulyuKFeWQqGgUjCKoq2trdEFg5/Ac/T09ER3jInP5wtGq6io4PF4kZpCpWDoPnUokY4SP336VCKREFYrlUrDFv/ly5ezsrLChg0NDVEsWK/X8/n8SGWx2ewopwGUSiWTySSgdu6rFRiwpaUltIlSqTTm2XwiZy3HxsYiVdbW1hbdVy6XQ49bR0EikZw8eTJSwOfPnyMIotFoHA7Hq1evYhZPRLBer8/NzQUro9Pp165di+l+5swZmUyGUW1TU1N/fz+BIiNB8Gc8x44dAxcn9kc8i8UiEomifKrzeLxbt241NDQQKy8KxH+3pFKpgvMslUplMhneCH6///79+6WlpTMzM8PDw1qt1mg08vn8urq679+/Ey4sOvjOaYVht9uvX7+enp5+4MCB+L2+pBZSgv+LLOgfW8aDpODFTlLwYicpeLHzvxP8D/Vhw3rZKOfbAAAAAElFTkSuQmCC\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGB size=80x80>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGB size=80x80>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFAAAABQCAIAAAABc2X6AAAD2UlEQVR4nO2bO0jrUBjH0/igiFgRH0WKaKmKtUvRQRwqCIKPCIIWXNTBRSrt4AMjOAiCODkWC5LZQSd1ULSCg4JgzVAfsxaVKFJCiaghuYP3lhJ7tdd8PTf33PPbzmfy/86vJz2kqTWpqkr9T9B/ewKoIcK4Q4RxhwjjDhHGHSKMO0QYd3Khgnw+X2lp6dXVVSgUUlW1pKQEKhkYFYLZ2dmcnBzTL9bX10FiswHMJd3Q0KAoSnLo9Xqj0ShIMjgwwgcHB5pKOBwGSQYHRjh1eX9XMQiG3qUTiURnZ2d7e/vu7i5UJtgu/QmKojQ3N09OTsZisT86cXNz8+joiKKoaDQ6MTHBsqz+yaAQvrm54Xl+aGjo2wmPj4+SJIFMJuuXtCiKjY2N+nMODw/1h1AIhAVBAFmc8fFx/SEUAmGHwzE1NaU/5/T0VH8IhWaXZhhmbm6Opr/fq6+vb2lpCWQyKDYtj8fj8Xh8Pt/9/X0mxx8fH2su4IuLC6jJoBB+x2q1Wq3WLw+LRCLT09OaE/f29qCmYbgbD7fbrdnk9vf3bTYbVL6xhCVJampq0hTPz88BWxhLeHBwkOf51MrCwsLAwABgC2MJ19XVaSqXl5ewLQwkzHHc8vJyaqWlpYXjONguBhKur6/XVHiez8/Ph+1iFGFZlr1eb2qlsLDw5OQEvJFRhG9vbzW3JRsbGy6XC7yRUYSrqqrGxsaSQ7PZfHd3l41G6O60vqS7u5um6WAw6HA47Hb78PBwNroYSJhhGIZhWJatrKzU80njcwwk/A7gXWRajPIeRgYRxh0ijDtEGHeIMO4QYdxBJ8zzfDAYBHyk/j0QfXgoKiqiaVoUxeLiYkVR4vE4mr4fQbHCLMsmEglRFCmKisfjoijOz88j6JsWFMIfv+n0+/0I+qYFhXAkEtFUampqEPRNCwphzeOL3Nzc1dVVBH3TgmLTam1tTR0qihIIBAKBQIanu1yunZ2dt7c3s9msfzIohEdHR1OHiqIIgpD56eFwOC8v7/n5GWQyMJd0eXn5J39dXFy02+06W7S1telMeAdGuLe3V1Pp6elJfm/gdDqrq6t1tpBlWWfCT0D+RXVkZMT0gYqKipmZmeQxZWVlFovl42EZIkkSyFRh3sMcxxUUFKysrKQWBUF4enpKHa6trbnd7lAotLW19fr6mmG40+nc3t5+eXkBmSrMCquqenZ21tXVpVmWjo4OqHwoTCrc74djsZjFYqmtrU1uwjab7fr6GiofBvCX8OHhob+/X5Zlv98PHq4fyBX+JyAPAHCHCOMOEcYdIow7RBh3iDDu/HfCPwD5s1Tgmmp/PAAAAABJRU5ErkJggg==\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGB size=80x80>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFAAAABQCAIAAAABc2X6AAAFKUlEQVR4nO2bWyjzYRzHf8+mvM6HyUISySkrlBYSEaMIoV3shppcLEraheTKcuXClRxKURRFDrkgbhyWmNOiUS7mlJoROR+2vRf/3r1rDq/t9/Dq7/+53e/57fv5P8//sGcbsVgs8JPg/e8AXw0nzHY4YbbDCbMdTpjtcMJshxNmO5ww2+GEvxN6vX58fPz29pZmUwuOq6urh4cHZJNXOTw8LCoqIoS0traenJzQaosV7u3tnZ6ephLFjtjYWPKHmpqa7e1tKm2xwiaTKTo6+vn5mUoaK7OzszKZjNggEAhmZmYmJiaQnbHCFoslLS2NEDI5OYnsc3Nz09TUZDAYSktLyRv09fUh34WCsEgkIoRUVlY6N3x9fT09PX1oaCg0NPRVST8/P6lUKhaLCSGurq7z8/OYtBSEmaACgcChUWdnZxkZGXV1dXw+31ZPKBQqFArmIPL5/MHBwaenJ4vFYjKZZmdnj4+PkWkpCPv4+BBC1Gr1B+vHxsYODg7Cw8PtZrK+vl4sFjNK+fn5hBCZTIaPZwcFYWaKlErl+2U3Nzfl5eUqlcrOMzEx0WAwDA8P2xYzwnt7e/h4dmCFLy8vmdylpaXvlHV3d4+Ojnp4eFg9c3Jy9Hp9c3Mzs2LtYIQlEgky3kuwwqenp4yASqX6Z/Hu7q5YLJZKpaurq+9XMsL19fXIeC9xQT6oBQQEpKamqtXqpKSkfxZHRUUtLS19vHlMTAwi2utQeJa+uLgAgISEBHwrOyIjI6n3pCBsMBgAIDExEd/KDnd3d+o9scJms/n8/Bw+Z4Y/A6yw0Wg0m80A8JFz+DuAFWbWM/ycGf6hwsHBwVFRUTTyfDpY4YODAwAoLi6mkMUGX19fAGCuDnTBCm9sbABASUkJhSw21NbWAsD19TXdtkBLuLCwkEIWG/b29gDAaDTSbQt44c3NTfiEJf19hdfW1uBHCW9sbLi4uOTl5VFJY0Wr1QIA8wxHF6zw6upqVlaWt7c3lTRWFhcX4dvOsEQioRLFlq6uLviewnq93tPTk0oUK1tbW42NjQAQEBBAtzPghanbPj8/63S6x8dHAMjNzaXbHACwOx6/fv1ybqBer29vb1er1czjVGBgYHFxcUFBwf7+vlQqBYCVlRWRSISM9wrILSKhUNjV1eXoqKKiore23a3U1dUhs70Kdkk7cX1eXl6+v78/Ojp6v+zo6MjX13dhYcHZaG+APGDJycmOzrBQKHx/bm3h8/lNTU0KhQKZ0wr2HGY+1jjE4eGhq6vrW6+6u7vzeDzrxwaz2axSqQAgOTk5LCwsMzPTyaB/wC5pJ4TfsZXL5R0dHVqtNjMzs7Oz0/YWUFlZWVZWFh8f71zOvyBXSFVVlaNLOj09/eXSLS8vT0lJubu7s63c39+XSCQ1NTXWsqCgIOT3aVhhpVLpqPDU1JRVwMvLa2RkRKlUmkymt+ovLy8HBgasQ0QiESYwVrilpcWJ21J1dXV2dnZISIhOp/tI/dPTk1arZYQbGhocj/kXrHB7e7sTwgKBwMPDw9FRGo2GEOLm5uboQFv+w0ULAIxGoxPbNxqNJiIigtljcRqssL+//2dstb1Kf3//3NwccnsUex/e2dnR6XTIJh9kbm4O3wQ7wycnJ0NDQ3K5HB/la8AK83i8i4uLnp4ejUZDJdCng7niMbS1tfH5fPzva74GCt8P+/j4ZGVlBQcH41t9ARSEKyoq4uLi8H2+BmLh/j/MbjhhtsMJsx1OmO1wwmyHE2Y7nDDb4YTZzm9Sj9JxiTycoAAAAABJRU5ErkJggg==\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(imgs.shape[0]):\n",
    "    display(transforms.ToPILImage()(imgs[i]).convert(\"RGB\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "8"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "import json"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "with open('joyokanji.json', 'r') as f:\n",
    "    data = json.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "character = ''.join(data['jp'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "with open('joyokanji.txt', 'w') as f:\n",
    "    f.write(character)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "img = Image.open('sample.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "img_tensor = to_tensor(img)\n",
    "img_tensor = (img_tensor + 1) / 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "arranged_img = to_pil_image(img_tensor)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}